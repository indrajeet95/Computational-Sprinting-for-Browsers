Mail from Christopher (10.29.17)
- Web Browsers should use Nathaniel's technique to speed page loads. Hotweb first, then NSDI.
- We need an approach to support multiple sprinting mechanisms all of which drain rom a global budget. (long term, perhaps ASPLOS)
- Sprinting with different classes of services (ideally MASCOTS)

Convo with Christopher (11.7.17)
- Web browsers actually act as a server. It keeps on serving requests. It can decide on which requests to speed up based on
computational sprinting. On mobile phones for instance, we can put a power budget on the browser. The easy way out would be
to limit the number of cores on which the browser runs. But that would slow down the browser. When and where to sprint needs
to be modelled on multi core machines.

Key words - James Mickens Harvard University (https://www.seas.harvard.edu/directory/mickens) -
Web performance and analysis - (https://mickens.seas.harvard.edu/research)

Convo with Chris (11.9.17)
- Generate a file that has the list of objects fetched and the respective time at which they are fetched.
This file is based on the dependency graph that can be formed using Scout, etc.
So basically if Polaris schedules the fetching of objects from the web page, Chris wants the result.
He wants me to implement Polaris and give it as a file. Or any other browser for that matter I believe.

I found out a paper "How Speedy is SPDY?". They have implemented epload that emulates the page load process for folks who
are only interested in running network experiments while hoping to preserve the authenticity of the page load process.
They also have a tutorial on how to set it up.

http://wprof.cs.washington.edu/spdy/tool/

So the obvious questions are:

What is SPDY?

SPDY (pronounced "SPeeDY") is a networking protocol whose goal is to speed up the web. SPDY augments HTTP with several
speed-related features that can dramatically reduce page load time:

1. SPDY allows client and server to compress request and response headers, which cuts down on bandwidth usage when the
similar headers (e.g. cookies) are sent over and over for multiple requests.
2. SPDY allows multiple, simultaneously multiplexed requests over a single connection, saving on round trips between client
 and server, and preventing low-priority resources from blocking higher-priority requests.
3. SPDY allows the server to actively push resources to the client that it knows the client will need
(e.g. JavaScript and CSS files) without waiting for the client to request them, allowing the server to make
efficient use of unutilized bandwidth.

The epload tool tries to compare the performance of http/1.1 and SPDY I guess. So it does not help us actually. We would want
to look at the scheduling mechanism.

There is a trace file which is offered in http://wprof.cs.washington.edu/spdy/data/. The command for viewing the file is as
follows:
tcpick -C -yP -r client-conn\=6-iw\=3-bo\=0-rtt\=10ms-bw\=10Mbits-plr\=0-fs\=100B-obj\=2-ts\=1373314195.pcap

On a sidenote, we can view in detail the time it takes for every request to be served in the google chrome devtools. However,
we don't what's the dependency graph it uses.
https://developers.google.com/web/tools/chrome-devtools/network-performance/reference#overview
https://developers.google.com/web/tools/chrome-devtools/network-performance/understanding-resource-timing

Ok. The sad part is all that we "found" in SPDY is actually given in the paper. We need to read Polaris more.

Convo with Christopher (16.11.2017)
After class I spoke to him. He told me to send the files.
There is something called har format which can be downloaded directly from the chrome dev tools and mozilla firefox network
monitor. These files can be directly viewed online using some extensions and sites like

https://chrome.google.com/webstore/detail/http-archive-viewer/ebbdbdmhegaoooipfnjikefdpeoaidml?hl=en
https://www.electricmonk.nl/log/2013/09/16/quick-n-dirty-har-http-archive-viewer/
http://www.softwareishard.com/har/viewer/

The final csv one which we use is http://www.yamamoto.com.ar/blog/?p=201. Details on how to do it is given in the link.

har2csv --in <source> --out <destination>
NOTE FROM THE MIDDLE: Sometimes if you get an error stating that you dont have heap space in java, find a better system than the one on which you are currently running the code. I tried manually setting the heap space but it doesnt work. It worked on Mohammad's laptop.

The contents of the mail sent to Chris is as follows:

Sir,

I have attached two files in the HAR (HTTP Archive) format that was scrapped using chrome dev tools and Mozilla Firefox
Network Monitor. The webpage that we are loading is http://web.cse.ohio-state.edu/~stewart.962/. These files were generated
by disabling the cache. The HAR files are further converted to CSV format. They contain the timestamp of when the object was
requested and the corresponding time taken to fetch the resource. They have been attached as well.

Thank you. 

Sincerely,
Indrajeet

Reply from Chris for the results (21.11.2017)

Indrajeet,

We need to sit and discuss this project for an hour or two.  When are you available?  Can you coordinate with Nathaniel to find 4 dates where
you are both available for 2 hours?  I will choose from them.

To be sure, this data is good.  In preparation for the meeting, you can look around for widely used traces of browser usage patterns (that is a sequence of URL accesses that can serve a benchmark).  You might also brush up on jquery and javascript.

Notes (for my memory):

a) We can use the data provided by sprinting to multiple cores
b) We can combine multiple HTTP queries into a multi-query context (possibly using Firefox plugins)
c) Offline profile arrival rate, arrival rate distribution, service time and service time distribution
d) Ship a simulator in the plugin to sweep over timeout settings for refresh rate
e) Since arrival rate of multi-query contexts will be low, marginal rate should work fine.

- Chris

Meeting with Chris and Nathaniel (1.12.2017)

Challenges in Sprinting
-> Change DVFS implies change browsers. So effectively will we able to design a product that is browser agnostic?

Currently we are looking at wide area and local area bottlenecks. The important concept is that the time that each request takes to be processed
is in the order of milliseconds, whereas, the overhead of deciding when to sprint and when not to takes seconds. Hence, sprinting in real time
using DVFS is not the ideal solution for browsers. However, there was a paper in HPCA where DVFS scaling was acheived at the millisecond level but
we are not sure how much effective would that be in improving the performance or reducing the battery consumption overall.

In the middle, Chris wanted me to design a plugin that keeps track of browser performance (linux based)
A major difference exists in the way chrome and firefox handle their requests. Chrome creates a huge number of threads for even one tab whereas
firefox kind of a process or thread per tab.

ps -T firefox grep (the wrong command but something along these lines)
Look at threads of firefox -> Assign tasks to each threads
Command for viewing battery usage in Ubuntu : upower -i /org/freedesktop/UPower/devices/battery_BAT0
um
One idea was to sprint whenever the service time or waiting time is taking too long. Nathaniel felt that this was more like doing bare minimum
rather than trying to improve the performance. Chris told that its about energy and making sure the power budget is under consideration.

The dependency chain can be viewed as a queueing delay chain for the computational sprinting aspect.

We had a side discussion of bringing Nathaniel's work at the OS level and scheduling the processes based on their running times. But this would 
not be general and it would be dependent on the applications. Effectively, what we would want is that when a process is scheduled, the number 
of cores on which it is to run will be specificed and a timeout after which sprinting will take place. The OS can take care of this scheduling 
thereby reducing the load on the design of the application.

Since, we cannot look at the millisecond level, we are going to group say 10 requests and call it a task and check the avg. processing time. Based 
on the histogram we can fix on the timeout.

Tasks
1. Design a mechanism where we can control the number of cores on which the threads of chrome run. There might be a config file where we can go ahead and change the values. So there should be some code which the service file in the OS which directly links both of these.
2. Get History of 500 pages and load them sequentially and get the results of the network activity from developer tools without actually opening it using javascript. We are looking a chrome plugin.
3. Try to calculate the battery usage for all these experiments using 1,2,..8 cores.

-------------
12/14/2017
Work for the TASK 1

ps -C chrome : View all the threads related to chrome
top -p xxxx : view details about process xxxx
pkill chrome : kill all the processes related to chrome
killall chrome : kills all the processes related to chrome but in a different manner
grep -c ^processor /proc/cpuinfo : Prints the number of cores

Google chrome PID using htop is 9242
taskset -cp xxxx: Provides the current afinity list for xxxx which is 0-3 for our system
taskset -cp 0,4 xxxx: Pin a running process with PID xxxx to CPU 0 and 4
 
Dedicate a Whole CPU Core to a Particular Program

While taskset allows a particular program to be assigned to certain CPUs, that does not mean that no other programs or processes will be scheduled on those CPUs. If you want to prevent this and dedicate a whole CPU core to a particular program, you can use "isolcpus" kernel parameter, which allows you to reserve the CPU core during boot.

Add the kernel parameter "isolcpus=<CPU_ID>" to the boot loader during boot or GRUB configuration file. Then the Linux scheduler will not schedule any regular process on the reserved CPU core(s), unless specifically requested with taskset. For example, to reserve CPU cores 0 and 1, add "isolcpus=0,1" kernel parameter. Upon boot, then use taskset to safely assign the reserved CPU cores to your program.

Steps to do the same
1. Edit /etc/default/grub and added isolcpus=0,1 to GRUB_CMDLINE_LINUX_DEFAULT
2. GRUB_CMDLINE_LINUX_DEFAULT="quiet splash isolcpus=0,1"
3. Run update-grub
4. Reboot
5. After that, cat /proc/cmdline reveals:
BOOT_IMAGE=/boot/vmlinuz-3.2.0-26-generic root=UUID=52cfedad-40be-41b9-9f88-c282a7ae3f24 ro quiet splash isolcpus=0,1 vt.handoff=7
6. Tested using stress: sudo apt-get install stress && stress -c 4
7. Monitored using top and pressing 1 to display individual CPU stats

Results when CPU0 and CPU1 are isolated
%Cpu0  :  0.0 us,  0.0 sy,  0.0 ni, 98.0 id,  2.0 wa,  0.0 hi,  0.0 si,  0.0 st
%Cpu1  :  0.0 us,  0.0 sy,  0.0 ni,100.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st
%Cpu2  : 99.3 us,  0.7 sy,  0.0 ni,  0.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st
%Cpu3  : 99.7 us,  0.3 sy,  0.0 ni,  0.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st

which implies that CPU 0 & 1 are isolated. Now we can use taskset to run chrome on CPU 0 or 1 or even 0 & 1 using the command "taskset 0x1 google-chrome" which runs on CPU 0.
Note: Convert the hexadecimal to binary. For example, the lowest bit in a hexadecimal bitmask corresponds to core ID 0, the second lowest bit from the right to core ID 1, the third lowest bit to core ID 2, etc. So for example, a CPU affinity "0x11 = 0b10001" represents CPU core 0 and 4.

Source : http://xmodulo.com/run-program-process-specific-cpu-cores-linux.html
https://askubuntu.com/questions/165075/how-to-get-isolcpus-kernel-parameter-working-with-precise-12-04-amd64

Comment: You don't have to run the isolate CPUs. Sir said so. So use taskset to run chrome on specific cores.
---------------------------------------------------------------------------------------------------------------------------------------------------
Work for TASK 2
12/14/2017

https://stackoverflow.com/questions/16210468/chrome-dev-tools-how-to-trace-network-for-a-link-that-opens-a-new-tab
chrome://net-internals/#sockets from 

Possible Solution
https://groups.google.com/forum/#!msg/google-chrome-developer-tools/3hY_wAgquvw/JcB70W7bCAAJ
https://developer.chrome.com/extensions/devtools_network
https://developer.chrome.com/extensions/webRequest -> https://developer.chrome.com/extensions/declare_permissions; https://developer.chrome.com/extensions/manifest

Build an own plugin that can export data from developer tools and store the results in a har file.

Available Plugins on chrome market
1. Network Sniffer
2. Network Record & Network Replay (Extension)

While designing a chrome plugin would be ideal way, we can always resort to a simple solution. We can run a userscript for loading 500 webpages sequentially and simulatenously open developer tools and record the activity. I found this userscript onine.

https://stackoverflow.com/questions/12260455/how-to-open-a-list-of-pages-automatically-and-sequentially

I suck. I did find some userscripts online but I was unable to make them work. I am giving up temporarily and looking at making a chrome plugin using javascript.

https://www.sitepoint.com/create-chrome-extension-10-minutes-flat/

I am currently working with a plugin that given a set of links loads them after a certain timeout
found out how to make it load in same tab
now have to figure out the onload thing. it is not working so far because cross origin issues
read the following links please
https://en.wikipedia.org/wiki/Cross-origin_resource_sharing
https://developer.mozilla.org/en-US/docs/Web/HTTP/CORS

https://stackoverflow.com/questions/3030859/detecting-the-onload-event-of-a-window-opened-with-window-open
The main problem for doing what we want to do.

I am not giving up but I need to do something so I am going to stop right here and try to load 500 web pages.

Trying the following
https://stackoverflow.com/questions/2797560/set-a-callback-function-to-a-new-window-in-javascript
with
https://coderwall.com/p/_ff9dg/enabling-cross-origin-resource-sharing-cors-on-google-chrome-for-javascript-api-calls-and-debugging-purposes
https://stackoverflow.com/questions/30040441/javascript-add-event-listener-to-document-opened-in-new-window
https://stackoverflow.com/questions/14265305/how-to-add-an-onload-eventlistener-to-a-popup-in-opera

1/5/2018

Randomly I did find a tool challed Charles which records the network activity for the whole system not just for chrome. It might come in handy in the future. I have installed and you can start it by using the command "charles".

Anayways, Preserving the log is now a problem which doesn't happen by default.
https://stackoverflow.com/questions/45133659/how-to-enable-preserve-log-in-network-tab-in-chrome-developer-tools-by-default

Each and every time we need to check the button so that it preserves. While there is a button in the preferences area it doesn't work.
Found a solution which has been documented in the code.
---------------------------------------------------------------------------------------------------------------------------------------------------
WORK FOR TASK3
1/7/2017

Calculate the battery usage throughout the process of testing of chrome

upower -i /org/freedesktop/UPower/devices/battery_BAT0 (Command sent by Chris)
powerstat: http://www.hecticgeek.com/2012/02/powerstat-power-calculator-ubuntu-linux/
powertop: http://linuxpitstop.com/install-powertop-on-ubuntu/

I was thinking that we could do this. Measure the average power consumption over the course of loading 500 web pages and also calculate the time 
it takes for the same. Get the energy by multiplying time and power and then we can compare the values based on the number of cores that are used to run chrome.

We have 441 pages to load and I am going to give 10 seconds for each page to load.
441*10 = 4410 seconds with 10 seconds intervals means 441 samples.

=> powerstat 10 441 would be ideal
=> powerstat 10 445 just for safety :p

If we have 400 pages to load and each page is given 12 seconds to load.
400*12 = 4800 seconds with 12 seconds intervals means 400 samples

=> powerstat 12 400 would be ideal
=> powerstat 12 405 just for safety :p

Power measurement will start in 180 seconds so we can start chrome in a new terminal with cores specified and start the experiment by then.
Our experiment will take 73.5 minutes to complete.

Hexa to Binary table
1 - 0001 *
2 - 0010
3 - 0011 *
4 - 0100
5 - 0101
6 - 0110
7 - 0111 *
8 - 1000
9 - 1001
A - 1010
B - 1011
C - 1100
D - 1101
E - 1110
F - 1111 *

=> taskset 0x1 google-chrome (for Running on CPU 0)
=> taskset 0x3 google-chrome (for Running on CPU 0 and CPU 1)
=> taskset 0x7 google-chrome (for Running on CPU 0, CPU 1 and CPU 2)
-> taskset 0xF google-chrome (for Running on CPU 0, CPU 1, CPU 2 and CPU 3)

NOTE: While running make sure that the amount of time you provide for each page to load matches with the powerstat command and also check if the battery of the system can hold up for that long.

http://www.copsmodels.com/gpeidl.htm
---------------------------------------------------------------------------------------------------------------------------------------------------
Get remaining battery using the code base in AUAV Linux
Develop a serperate plugin matches the url. So essentially we will have a list of websites that need to be experimented on. The chrome plugin 
should always be working. When the url matches, record the time it takes to load the website from developer tools.
Look into chromium browser.

17.1.2018
Meeting with Nathaniel
https://wiki.archlinux.org/index.php/CPU_frequency_scaling
The reason for the random values that you observe in the power results is because the cores can do what they want. run at any frequency. so you dont have actual control over whats happening. maybe when you pin chrome to particular cpu with a specific frequency then you can see results that you are expecting.

24.1.2018
Meeting with Nathaniel and Chris
It was a random meeting. Not much of preparation. He wanted me to create the trace files using excel or something. need to figure that out asap.
So essentially, the remaining tasks are to develop the new chrome plugin that loads 10 pages at the same time.

https://github.com/sitespeedio/browsertime

So, we did find this Web Performance TIming api. that works. we need to do the following i guess :p
1. trigger the extension when the url matches so and so links DONE
2. write the contents to a file with the url also to the local system (https://www.html5rocks.com/en/tutorials/file/filesystem/)
3. Check the results of power consumption when you toggle with the frequency scaling that Nathaniel did.
4. Make the traces using zipf distribution
5. Connect to Chris10 (saravanan.8 and Nitt@1000) DONE 

ssh saravanan.8@chris10.cse.ohio-state.edu
Nitt@1000

We are currently lookin at the Zipf distribution
I just read that the internet traffic follows the zipf distribution
https://www.nngroup.com/articles/zipf-curves-and-website-popularity/

However I also found one more article which claims that it follows SE distribution
The name of the paper being sigmetrics.pdf (FROM OSU)

So lets get to business. I have apparently been using Moz Top 500 list and not Alexa Top 500 list because it's paid. Anyways Alexa has a feature which states the no of unique visits from one user to each site in top 500. I think we need to rank the sites based on these values. And we can start our narrative by saying a user visits Google 8.34*365 times in a year. Every time it takes 3 seconds to load. But if he uses our method
It takes 1.5 seconds to load. So essentially he is saving 1.5*8.34*365 seconds which is 76 minutes. An user will minimum visit 500 pages like Google. So he is saving 76 minutes for 1 page. SO 500*76 mintues he will be saving which transalates 38000 minutes = 633 hours = 10 days.

This is flawed anyways.

Just created a excel sheet which is onthe desktop and it does contain and the parameter is 2. However i dont think this is right per user.
we need to follow some other function when it comes to per user. 

------------------------
25/2/2018

Ok. I am working with the Web Performance API and I did find a chrome plugin that uses that to display the time and everything. 
I figured we need a content script that runs on the background because thats what chris wants. 
Made that work using this link and a lot of others but i have to admit that this link was the one to help me out finally.

THE ONE WHO SOLVED IT FOR ME LIKE FIRST TIME IT WAS The SOLUTION
https://stackoverflow.com/questions/15664991/why-does-my-attempt-at-measuring-render-time-with-web-performance-api-keep-resul

Now working on making the download work automatically every time it pages in manifest file.
I am able to download the details into a text file each time the page is loaded. we should figure out a way to combine them together which shouldnt be a problem.
and then put everything in a csv file which again shouldnt be a problem if we just seperate the values using commas.

MEETING AGENDA WITH CHRIS

WHAT DO I HAVE?
-> A plugin that runs a content script which matches the URL to the Manifest file and triggers the script. It downloads the page load time and other activites measurements for each page locally.
-> A plugin that can has a textbox which when provided with a list of web pages to load, loads them sequentially on the same tab and keeps track of the network activity since the developer tools shall be open which can be later used to download the file.

I have a huge list of trace patterns that follow the Zipf distribution
I need to get the page load times for each of them and also the power consumption on varying cores and frequency
I need to run them on Chris's machine since we cannot possibly load them on my systems because it would die
I need to find out find power consumed because Chris's system is always powered

FOLLOW UP OF SETTING FREQUENCIES OF CORES

Your system uses new driver called intel_pstate. There are only two governors available when using this driver: powersave and performance.
The userspace governor is only available with the older acpi-cpufreq driver (which will be automatically used if you disable intel_pstate at boot time; you then set the governor/frequency with cpupower):

1. disable the current driver: add intel_pstate=disable to your kernel boot line
Follow the below steps to do the above task
https://askubuntu.com/questions/19486/how-do-i-add-a-kernel-boot-parameter

2. boot, then load the userspace module: modprobe cpufreq_userspace
3. set the governor: cpupower frequency-set --governor userspace
4. set the frequency: cpupower --cpu all frequency-set --freq 800MHz
5. cpufreq-info to check if all the frequencies are the same as the ones you set

Now I have set everything at 1.5 GHz

We need to get the same results that we got for the previous experiments

I have a shell script that can load the web pages on chrome every certain number of seconds that I provide and using the cotent script we can also get the results of each page load. We can run it on the server. the only prob is to install the content script and finding out how to measure power in the server.

28.2.2018
Right the presentation is over
It was good. We only presented the results section.
INPUTS for NEXT ITERATION of RESULTS
-> Find Energy. he is cool with battery. Jason suggested something about finding the voltage at which the server runs and something like that.
We had a issue with servers because they are always in power and as of now we can only measure stuff when its discharging. SOLVED
-> Find how to install the content script onto chrome using x11 forwarding DONE
-> open mulitple tabs or windows with random times. Use an exponential distribution to get values and load them at that time. This is easy. you should write the function in nice.sh and deploy it.
we need to open a new window which will have a process id so we can open that. We also will record the time at which the query starts and then based on a threshold we will decide if we gonna sprint or not.

awk '{ print ",\"*://*." $0 "*\"";}' Top\ 500\ Pages > temp.txt


google-chrome -load-extension=/home/saravanan.8/Content

you need to measure energy so we need to multiply power and time.
So we need to measure power.

https://github.com/kentcz/rapl-tools

What are you gonna do???
It's not possible to do it on the server of issues such as killing chrome once the file has been downloaded, etc.
So we are going to run it on our system. That's fine.

So we have fixed the frequency of the cores. Just check once if they are still at 1.5 GHz. THey are not. But i just set them all to 1.5 GHz.

Now we are going to run the shell script for 500 web pages which is essentially going to load 500 new windows which is lot to take and we are going to run it on just one core and increase it gradually and at the same time measure power and store the contents onto a CSV file.
Cool.

added taskset to script.
that works.
We are gonna use ./AppPowerMeter to measure the power which stores the results onto rapl.csv file


13.3.2018
If we are going to run these experiments on the Server then we have to know all about the system.
It's CentOS 6.9 with kernel 2.6
uname --help does it all.

We have to discuss what kind of pages are we going to load.
Its 50 pages. 

Set at 1600 MHz all cores and sleep for 5 seconds constant
First exp : 4 cores
Second Exp : 3 cores
Third Exp : 2 cores
https://stackoverflow.com/questions/21470362/find-the-pids-of-running-processes-and-store-as-an-array
https://stackoverflow.com/questions/1908610/how-to-get-pid-of-background-process
https://serverfault.com/questions/71360/find-and-kill-old-processes
https://serverfault.com/questions/181477/how-do-i-kill-processes-older-than-t
https://serverfault.com/questions/71360/find-and-kill-old-processes?noredirect=1&lq=1

FIRST SET
Round 1
1 Core; 5 seconds; 
Round 2
2 Coresl 5 seconds; 
Round 3
3 Cores; 5 seconds;
Round 4
4 Cores; 5 seconds;

SECOND SET
Round 1
1 Core; 10 seconds;
Round 2
2 Coresl 10 seconds; 
Round 3
3 Cores; 10 seconds;
Round 4
4 Cores; 10 seconds;


So i fucked up with deleting all the files. I am pasting some stuff so it will help in the future.
 awk '{ print ",\"*://*." $0 "*\"";}' Top\ 500\ Pages > temp.txt
https://stackoverflow.com/questions/14964035/how-to-export-javascript-array-info-to-csv-on-client-side
https://stackoverflow.com/questions/19347499/chrome-extension-simple-content-script-for-running-js-on-any-page


FEB 23rd 5:20 pm look for history
this close to being done

FRESH LIFE
Sir wants reliable results.
SO here we go....

Steps to do the experiment
Once you switch on the system do the following...

1. modprobe cpufreq_userspace
2. sudo cpupower frequency-set --governor userspace
3. sudo cpupower --cpu all frequency-set --freq 1600MHz
4. cpufreq-info

Before using the rapl-tools, you will need to load the msr module and adjust the permissions.
1. sudo modprobe msr
2. sudo chmod o+rw /dev/cpu/0/msr
3. sudo apt-get install libcap2-bin
4. sudo setcap cap_sys_rawio+ep ./AppPowerMeter
5. sudo setcap cap_sys_rawio+ep ./PowerMonitor

Btw I guess all this while I have been giving wrong results due to the fact that I did not switch off the turboboost. 
So now when I run the experiements with even 10 seconds gap the chrome is crashing and I cant seem to saveit. I believe that using web sql database I can make things work because we dont need to download data as a text file which actually works like opening a new href or tab. 

https://www.tutorialspoint.com/html5/html5_web_sql.htm
https://stackoverflow.com/questions/24624451/adding-values-dynamically-to-the-table-in-web-sql-database-using-javascript

https://github.com/knadh/localStorageDB

//////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
window.onload = function () {
	window.setTimeout(function () {

	var p = {};
	p.prerequestTime = performance.timing.requestStart - performance.timing.navigationStart;
	p.latencyTime = performance.timing.responseStart - performance.timing.requestStart;
	p.serverTime = performance.timing.responseEnd - performance.timing.responseStart;
	p.domLoadingTime = performance.timing.domInteractive - performance.timing.responseEnd;
	p.domCompleteTime = performance.timing.domComplete - performance.timing.domInteractive;
	p.loadTime = performance.timing.loadEventEnd - performance.timing.domComplete;
	p.onloadTime = performance.timing.loadEventEnd - performance.timing.navigationStart;
	
	query = window.location.href;
	
	var answers = query + '\nPre-Request: ' + p.prerequestTime + '\nLatency: ' + p.latencyTime + '\nServer: ' + p.serverTime + '\nDOM Loading' 		+ p.domLoadingTime + '\nDOM Complete: ' + p.domCompleteTime + '\nLoad: ' + p.loadTime + '\nTotal Time: ' + p.onloadTime;
	
	//alert(answers);
	
	var db = openDatabase('mydb', '1.0', 'Test DB', 2 * 1024 * 1024);
	
	db.transaction(function (tx) {
		tx.executeSql('CREATE TABLE IF NOT EXISTS LOGS_5 (id INTEGER PRIMARY KEY AUTOINCREMENT, name, prerequest, latency, server, domloading, domcomplete, load, total)');
		var insertStatement="INSERT INTO LOGS_5(name, prerequest, latency, server, domloading, domcomplete, load, total) VALUES(?,?,?,?,?,?,?,?)";
		tx.executeSql(insertStatement, [query, p.prerequestTime, p.latencyTime, p.serverTime, p.domLoadingTime, p.domCompleteTime, p.loadTime, p.onloadTime]);
	alert("Log message created and row inserted.");
	});

	//var hiddenElement = document.createElement('a');
	//hiddenElement.href = 'data:attachment/text,' + encodeURI(answers);
	//hiddenElement.target = '_blank';
	//hiddenElement.download = Date.now() + '.txt';
	//hiddenElement.click();

     }, 0);
 }
///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
22.3.2018
Working on providing multiple results

# Usage: ./nice.sh file_name
#!/bin/bash
# RANDOM=100
while IFS='' read -r line || [[ -n "$line" ]]; do
	taskset 0x3 google-chrome --load-extension=/home/indrajeet/Downloads/Content --new-window "$line" &>/dev/null &
	temp_pid=($(ps -eo pid,etime,comm | awk '$3~/chrome/ {print $1}'))
	for pid in `ps -eo pid,etime,comm | awk -v h1=${temp_pid[0]} -v h2=${temp_pid[1]} -v h3=${temp_pid[2]} -v h4=${temp_pid[3]} -v h5=${temp_pid[4]} '$1!=h1 && $1!=h2 && $1!=h3 && $1!=h4 && $1!=h5 && $2~/^00:30/ && $3~/chrome/ {print $1}'` ; do kill -15 $pid ; done
	sleep 5
	# sleep $(($RANDOM%20 + 5))
done < "$1"
///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////


Run experiments for Core 1 and 3 && Core 0 and 2
which means taskset 0x5 for Core 0 and 2
which means taskset 0xA for Core 1 and 3

Lets see how results fare.
Record timestamp as to when the first script stops.
?
:)

Also sir felt that rather than the finish time of each experiment just give me the time at which the last request was sent and that should do so basically i should run all that shit again. 

anyways just if my results become useful from the past, i have them in /extra work/results/huge collection/ 
there is a ppt which has all the tables and all the finish times are for the last web page to load. so i manually stop them.

i am gonna try to get better results now let see how it goes.


Median avg load time 95th percentile for all sets of outputs multiples of 5
FInd the time at which each request is sent
Find the time at which the request is starting to process - essentially find out the loca ltime also and make sure that the difference is minimal
that is if you have a ot of queries then it is going to queued -> queueing delay

Computational sprinting using taskset to assign more cores if it goes over a certain time
we will focus on cores and then on changing the frequency


Basically we need to make sure we are getting the correct values for load time and everything else. I believe using the indexeddb might just solve it
lets see
https://gist.github.com/BigstickCarpet/a0d6389a5d0e3a24814b

////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
INDEXEDDB


window.onload = function () {
	window.setTimeout(function () {

	var p = {};
	p.prerequestTime = performance.timing.requestStart - performance.timing.navigationStart;
	p.latencyTime = performance.timing.responseStart - performance.timing.requestStart;
	p.serverTime = performance.timing.responseEnd - performance.timing.responseStart;
	p.domLoadingTime = performance.timing.domInteractive - performance.timing.responseEnd;
	p.domCompleteTime = performance.timing.domComplete - performance.timing.domInteractive;
	p.loadTime = performance.timing.loadEventEnd - performance.timing.domComplete;
	p.onloadTime = performance.timing.loadEventEnd - performance.timing.navigationStart;
	
	query = window.location.href;
	
	var answers = query + '\nPre-Request: ' + p.prerequestTime + '\nLatency: ' + p.latencyTime + '\nServer: ' + p.serverTime + '\nDOM Loading' 		+ p.domLoadingTime + '\nDOM Complete: ' + p.domCompleteTime + '\nLoad: ' + p.loadTime + '\nTotal Time: ' + p.onloadTime;
	
	//alert(answers);
	
var indexedDB = window.indexedDB || window.mozIndexedDB || window.webkitIndexedDB || window.msIndexedDB || window.shimIndexedDB;

// Open (or create) the database
var open = indexedDB.open("MyDatabase", 1);

// Create the schema
open.onupgradeneeded = function() {
    var db = open.result;
    var store = db.createObjectStore("MyObjectStore", {keyPath: "id"});
    var index = store.createIndex("NameIndex", ["name.last", "name.first"]);
};

open.onsuccess = function() {
    // Start a new transaction
    var db = open.result;
    var tx = db.transaction("MyObjectStore", "readwrite");
    var store = tx.objectStore("MyObjectStore");
    var index = store.index("NameIndex");

    // Add some data
    store.put({id: query, name: {first: "John", last: "Doe"}, age: 42, prerequest: p.prerequestTime});
    //store.put({id: 67890, name: {first: "Bob", last: "Smith"}, age: 35});
    
    // Query the data
    var getJohn = store.get(query);
    //var getBob = index.get(["Smith", "Bob"]);

    getJohn.onsuccess = function() {
        alert(query);
        alert(getJohn.result.prerequest);
	//console.log(getJohn.result.name.first);  // => "John"
    };

    //getBob.onsuccess = function() {
//	alert(getBob.result.name.first);
  //      console.log(getBob.result.name.first);   // => "Bob"
    //};

    // Close the db when the transaction is done
    tx.oncomplete = function() {
        db.close();
    };
}	
	//var hiddenElement = document.createElement('a');
	//hiddenElement.href = 'data:attachment/text,' + encodeURI(answers);
	//hiddenElement.target = '_blank';
	//hiddenElement.download = Date.now() + '.txt';
	//hiddenElement.click();

     }, 0);
 }
//////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

IndexDB not working
Websql not working
localstorage not working because of same domain issues

/////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

window.onload = function () {

	window.setTimeout(function () {

	var p = {};
	p.prerequestTime = performance.timing.requestStart - performance.timing.navigationStart;
	p.latencyTime = performance.timing.responseStart - performance.timing.requestStart;
	p.serverTime = performance.timing.responseEnd - performance.timing.responseStart;
	p.domLoadingTime = performance.timing.domInteractive - performance.timing.responseEnd;
	p.domCompleteTime = performance.timing.domComplete - performance.timing.domInteractive;
	p.loadTime = performance.timing.loadEventEnd - performance.timing.domComplete;
	p.onloadTime = performance.timing.loadEventEnd - performance.timing.navigationStart;
	
	query = window.location.href;
	
	var answers = query + '\nPre-Request: ' + p.prerequestTime + '\nLatency: ' + p.latencyTime + '\nServer: ' + p.serverTime + '\nDOM Loading' 		+ p.domLoadingTime + '\nDOM Complete: ' + p.domCompleteTime + '\nLoad: ' + p.loadTime + '\nTotal Time: ' + p.onloadTime;

var addItem = function (query_1, prerequest_1, latency_1) {
    var oldItems = JSON.parse(localStorage.getItem('itemsArray')) || [];
    
    var newItem = {
        'link': query_1,
        'prerequest': prerequest_1,
        'latency': latency_1
    };
    
    oldItems.push(newItem);
    
    localStorage.setItem('itemsArray', JSON.stringify(oldItems));
};

console.log(JSON.parse(localStorage.getItem('itemsArray')));

addItem(query, p.prerequestTime, p.latencyTime);

	// Put the object into storage
	//localStorage.setItem('testObject', JSON.stringify(testObject));

	// Retrieve the object from storage
	//var retrievedObject = localStorage.getItem('testObject');

	//console.log('retrievedObject: ', JSON.parse(retrievedObject));	
	//alert(answers);
	//var hiddenElement = document.createElement('a');
	//hiddenElement.href = 'data:attachment/text,' + encodeURI(answers);
	//hiddenElement.target = '_blank';
	//hiddenElement.download = Date.now() + '.txt';
	//hiddenElement.click();

     }, 0);
 }
/////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////


Something to realize apparently
I'm trying to use chrome.storage.local in my extension, and it doesn't seem to work. I used localStorage but realized that I can't use it in content scripts over multiple pages.

ALERT: All four options like WEBSQL, INDEXEDDB and what not are origin based so it will never work for us
https://www.html5rocks.com/en/tutorials/offline/storage/

WebSQL : Cross origin issues
IndexedDB : Cross origin issues
Web Local Storage : Cross origin issues
Chrome Storage : too simple to use only supports key value pairs
Logging : gets overwritten for every new window
Text File Download : Creating problems of data not being consistent
File System API : Sandboxed Environment

/////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

window.onload = function () {

	window.setTimeout(function () {

	var p = {};
	p.prerequestTime = performance.timing.requestStart - performance.timing.navigationStart;
	p.latencyTime = performance.timing.responseStart - performance.timing.requestStart;
	p.serverTime = performance.timing.responseEnd - performance.timing.responseStart;
	p.domLoadingTime = performance.timing.domInteractive - performance.timing.responseEnd;
	p.domCompleteTime = performance.timing.domComplete - performance.timing.domInteractive;
	p.loadTime = performance.timing.loadEventEnd - performance.timing.domComplete;
	p.onloadTime = performance.timing.loadEventEnd - performance.timing.navigationStart;
	
	query = window.location.href;
	
	var answers = query + '\nPre-Request: ' + p.prerequestTime + '\nLatency: ' + p.latencyTime + '\nServer: ' + p.serverTime + '\nDOM Loading' 		+ p.domLoadingTime + '\nDOM Complete: ' + p.domCompleteTime + '\nLoad: ' + p.loadTime + '\nTotal Time: ' + p.onloadTime;

	console.log("Its working");
//	chrome.storage.sync.set({prerequest: p.prerequestTime}, function () {
//	console.log("had saved!");
//	chrome.storage.sync.get("prerequest", function (value) { console.log("read value: ", value.prerequest); } );
//	});
//	chrome.storage.sync.set({queryf: query}, function () {
//	console.log("had saved!");
//	chrome.storage.sync.get("queryf", function (value) { console.log("read value: ", value.queryf); } );
//	});
	//alert(answers);
	//var hiddenElement = document.createElement('a');
	//hiddenElement.href = 'data:attachment/text,' + encodeURI(answers);
	//hiddenElement.target = '_blank';
	//hiddenElement.download = Date.now() + '.txt';
	//hiddenElement.click();

	}, 0);
}
/////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////


Wow. thats a lot of time I havent told you what I have been doing.

First of all I want to thank a guy for his answer because for a common dude the answer wouldnt have been that clear but here it goes. 

How do I save data gathered by a chrome extension to my local system?

While the file system API should work. I have used a server approach too. You can setup a simple PHP/python server on the localhost and configure it to listen to GET/POST requests (depending on the type of data).

You can then setup your extension to capture data and hit the local server url. And you can save this data any way you like on the server. Since chrome will not allow you to send XHR from content_scripts you can use message api to send the data to background and then use xhr to send out requests.

In case you're not willing to use the background page, you will need to start chrome from the terminal using the no security flags (google them) this would disable the xhr checks.

Hope this makes sense.

https://www.quora.com/How-do-I-save-data-gathered-by-a-chrome-extension-to-my-local-system

I did the whole shiz. 
http://www.openjs.com/articles/ajax_xmlhttp_using_post.php

and now its working. but have to check in bulk. IT WORKS.


STEPS
1. Restart Apache: 
gksu gedit /etc/apache2/apache2.conf
/etc/init.d/apache2 restart

2. Edit connect.php file if needed:
sudo gedit /var/www/html/connect.php 

NOTE: https://developer.mozilla.org/en-US/docs/Web/API/PerformanceTiming for more details on what we are computing 
NOTE: https://www.w3.org/TR/navigation-timing/#sec-navigation-timing-interface search for the image to see what we are computing

3.  ps -eo pid,comm | awk '$2~/chrome/ {print $1}' : Command to find out if any stupid chrome processes are still running

So we have something going on. I am not even sure what I want.
But I am able to get good results bro. thats for sure. I am going to print the time stamp when the script calls taskset every time. I am also going to record the time stamp when the entry comes into the database. Essentially i dont see any difference because if you are gonna subtract both you would get the total time that we already storing in the database. Lets just make it sure. By the way we atleast know whether we will be wrong or not.

WE Need TO SPRINT
Call Taskset when a query takes longer than K seconds. Change Task set after K seconds for all processes.
We can't find out if a certain query takes more than a certain time just by using its respective PID. Because the PID will exist even if the page is done loading. 

1 query -> 1 PID -> 

Essentially we can only find out a page is taking so much time to load from the data that we are getting from the plugin. 
Its not simple by just running a script 
#the plugin needs to send a message to the script which in turn will launch all further queries with more cores. 
#We need to communicate between the script and the extension


https://stackoverflow.com/questions/14716659/taskset-python

https://unix.stackexchange.com/questions/30370/how-to-get-the-pid-of-the-last-executed-command-in-shell-script
https://stackoverflow.com/questions/25050311/extract-first-item-of-each-sublist-in-python
https://www.thegeekstuff.com/2013/06/python-list/?utm_source=feedly
https://stackoverflow.com/questions/17856928/how-to-terminate-process-from-python-using-pid

Yo. WHen I use subprocess to launch google chrome and then also save the pid to an array. and simultaneously print the pids related to google chrome, some of the pids stored in the array are missing. I dont know how that is happening.

Meeting on May 10th
WAIT
#the time needs to exponentially increasing increasing and not with constant time intervals like 5 seconds
#he wanted to move everything to python

# Move to python and dont use the for loop and use the taskset to get the PIDS  rather than fetching them everytime
# Allthe results with exponential arrival etime
# https://stackoverflow.com/questions/19152067/execute-linux-command-and-get-pid
# Interrupt and which looks like a callback rather than sleep
# https://stackoverflow.com/questions/27694818/interrupt-sleep-in-bash-with-a-signal-trap
# sprinted -> pid1 pid2 pid3,.... etc and dont sprint it again use PYTHON
# pid1 -> estimated start time note them down
keep note of power energy and dont exceed budget
yeah

#you want to check for sprinting and killing in the same command 

What can you do with taskset
1. you can view the CPU affinity of a running process
2. you can assign a running process to certain number of cores
3. you can launch a process on a set number of cores
and theres more but you can use this for now indrajeet saravanan.

RAPL : Running Average Power Limit

https://stackoverflow.com/questions/46725365/why-i-cant-use-perf-event-modifiers-with-power-energy-cores

Hey Nathaniel. I am facing a new issue with rapl-tools. I was already using the following tool for measuring energy and power for the project. 
https://github.com/kentcz/rapl-tools

Since I am trying to do everything from a single python script, I came across this command "perf stat -a -e power/energy-pkg/,power/energy-cores/,cycles -I 1000 sleep 1000" which outputs the energy consumed. But the values from these two tools vary in terms of thousands. The command says the energy consumed per second is 8 J whereas 


---------------------
Ways to measure power
1. Perf command on linux machines

sudo perf stat -a -r 1 \
    -e "power/energy-pkg/" \
    -e "power/energy-cores/" \
    -e "power/energy-gpu/" \
    -e "power/energy-ram/" \

OUTPUTS
Didn't fix the frequencies of the cores of the machine

Performance counter stats for 'system wide':

             83.80 Joules power/energy-pkg/                                           
             10.33 Joules power/energy-cores/                                         
              2.45 Joules power/energy-gpu/                                           
             15.71 Joules power/energy-ram/                                           

      19.513408441 seconds time elapsed

Fixing the frequency of the cores of the machine to 1.6 GHz

 Performance counter stats for 'system wide':

          1,853.55 Joules power/energy-pkg/                                           
            789.49 Joules power/energy-cores/                                         
            174.60 Joules power/energy-gpu/                                           
            297.97 Joules power/energy-ram/                                           

     204.262354119 seconds time elapsed

2. rapl-tools for checking consistency
./AppPowerMeter

OUTPUT
Avg Power: 9.834 W
Energy consumed : 2041.27 J

IMPORTANt LINKs TO READ
POWER PROFILING:::::::::::::::::https://developer.mozilla.org/en-US/docs/Mozilla/Performance/Power_profiling_overview
PERF:::::::::::::::::::::::::https://developer.mozilla.org/en-US/docs/Mozilla/Performance/perf
TOOLS/POWER/RAPL::::::::::::::::::::::::https://developer.mozilla.org/en-US/docs/Mozilla/Performance/tools_power_rapl


CONCLUSION
For the past few days, we have been thinking about how to measure power reliably. We were using someone's git code to measure power and storing them in a csv file. Then we came across this 'perf' command which makes things easier because its just command line. 
WE HAVE SEEN THAT BOTH THE RESULTS ARE ALMOST THE SAME HENCE WE CAN GO AHEAD AND PUT THIS COMMAND IN OUR SCRIPT.

I am facing the dumbest of issues man. I cant seem to format the output the way i want it. I need to get just the energy in joules to printed in each line and then maybe store it in an array and then keep a counter to keep on adding to them and hence being in control the energy budget.

append each line into file -> read file recursively and store the values into counter

STACKOVERFLOW ANSWER

You can use -I specific perf option to print the results every 1,2,3,etc seconds.

    perf stat -a -r 1 -e power/energy-pkg/ -I 1000

This command prints system-wide statistics of the energy-pkg plane every 1 second. You can find more examples in the following link: http://www.brendangregg.com/perf.html
------------------------------------------------
SOLUTION Suggestion by SHREY
Have a major script calling these two scripts which will essentially make the return value of energy visible to the script calling google chrome and you can use that as power budget.

Just use a variable to check for return.

we need to get results now. lets try to get some meaningful stuff. but we still have not done the part where once we exceed the power budget what happens. I think we need guidance for that on what to do. we will try to manage with some base results for this meeting.
**************************************************
1. modprobe cpufreq_userspace
2. sudo cpupower frequency-set --governor userspace
3. sudo cpupower --cpu all frequency-set --freq 1600MHz
4. cpufreq-info
***************************************************

22.5.18
we are here in search of a paper. All we need to do is get some nice results. Lets do this. 
We need a master script which passes the timeout and everything as arguments so that we can get results.
We are sure that the taskset works because it says it's doing its work. Don't have to worry about SYS_CAP_NICE permissions.
Exponential arrival time doesn't seem to be consistent meaning according to the definition the scale paramater and the mean of variables generated will be same. There seems to be a lot of deviation. But it's okay.
http://bigdata-madesimple.com/how-to-implement-these-5-powerful-probability-distributions-in-python/

All variations in terms of cores
1 base core - No sprinting
1 base core - Sprinting with 1 more core
1 base core - Sprinting with 2 more cores
1 base core - Sprinting with 3 more cores

Timeout variations
0,2,4,6,8,10,12,14

Avg. of Inter-arrival time in the exponential function variations
2.6,4.4,6.6,7.9,8.4

Budget for energy in Joules variations
200,600,1000,1500,1800,1900

Number of times each config needs to be run
10 times


WHAT ARE you doubtful about?
1. The manner in which I am collecting energy. I am just doing for some plane. Is this cool or we need to compute how much chrome only consumes?
2. I only sprint the pids greater than 10 in my list of chrome pids. If I sprint the lesser than pids they result in all the pids being created in the future to be sprinted in advanced. I found the reason. The CPU affinity of the child is inherited from the parent. So essentially 

pstree -p shows you how chrome processes are structured and there are close to 5 processes from which all the other pages branch. When you change the their CPU affinity mask then all other pages loaded will load with that setup. 

I will tell you why you wouldnt want to do that. Because you would be sprinting essentially from the minute you start. what about the budget???
you are not saving energy by sprinting the parent you fool. you are only wasting energy because all pages even the most fastest one will start with 4 cores instead of one because you are speeding up the parent. 

your only point of concern could be. Rather than sprinting each process seperately, if you speed up the parent you are saving the number of times of sprinting. This might be useful because the energy consumed for each step of sprinting a pid is unknown. BUT i can say for certain that it cant be more than the amount of energy you are saving by loading pages for 10 seconds normally and then sprinting them. 

24.5.18
CURRENT EXPERIMENTS

NULL : Not applicable

1 base core - No sprinting
1 base core - Sprinting with 3 more cores

Number of times each config needs to be run: 1 time

Budget for energy in Joules variations
500,3000,4000,5000,30000 or in other words no Budget

Timeout variations
0,4,8,12,16

Avg. of Inter-arrival time in the exponential function variations
2.6,4.4,6.6,7.9,8.4

100 web pages

LETS START MANNNNNNNNNNNNNNNNNNN

Let make sure loading 100 pages with all cores and with sprinting actually works because only 84 pages are the max amount of finishings i have senn
that would be your first step. Haha. Late reply bro. It doesnt work like that. Maximum I have seen is 92 pages I guess. Some end up getting failed but I dont think thats an issue.

